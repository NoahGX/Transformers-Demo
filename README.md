# Transformers-Demo

## Overview
In this Jupyter notebook, we demonstrate two key concepts related to Transformer and Transformer-Based models:  
  - A step-by-step implementation of the **multi-head attention** mechanism in a Transformer encoder, focusing on the basic computation of Q, K, V, attention scores, and weighted outputs.
  - An exploration of various **transformer-based NLP models** from the [Hugging Face Transformers](https://github.com/huggingface/transformers) library for multiple natural language processing tasks.  

## Features
  - Clear Python code illustrating how to calculate the **Query (Q)**, **Key (K)**, and **Value (V)** matrices.
  - Demonstration of attention score computation and softmax normalization.
  - Examples using **Hugging Face Transformers** for various NLP tasks such as text classification, sentiment analysis, etc.  

## Usage

## Prerequisites
- Jupyter Notebook

## Input

## Output

## Notes
