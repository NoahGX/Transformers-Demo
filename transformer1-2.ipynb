{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PjpuxXR8t7m"
   },
   "source": [
    "# **Transformer and Transformer-Based Models (Part 2)**\n",
    "\n",
    "In this python notebook, we will play with the transformer-based models provided in **transformers** for multiple natural language processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "07iMXEUK8t7r"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IA60pT4u8t7t"
   },
   "source": [
    "***\n",
    "\n",
    "## **1. Play with Transformer-Based Models**\n",
    "\n",
    "### 1.1 ~ Installation\n",
    "\n",
    "First, we install the *transformers* package with the following command:\n",
    "```\n",
    "pip install transformers\n",
    "```\n",
    "\n",
    "After it is done, we can load some pretrained BERT models and tokenizers like this (ignore warnings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "XHX5F-6t8t7u"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvvIhQyK8t7u"
   },
   "source": [
    "### 1.2 ~ Tokenizing Inputs\n",
    "\n",
    "Next, we will the following examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "AIaKtTur8t7v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "torch.Size([1, 275])\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The hotness of the sun and the coldness of the outer space are inexhaustible thermodynamic\n",
    "resources for human beings. From a thermodynamic point of view, any energy conversion systems\n",
    "that receive energy from the sun and/or dissipate energy to the universe are heat engines with\n",
    "photons as the \"working fluid\" and can be analyzed using the concept of entropy. While entropy\n",
    "analysis provides a particularly convenient way to understand the efficiency limits, it is typically\n",
    "taught in the context of thermodynamic cycles among quasi-equilibrium states and its\n",
    "generalization to solar energy conversion systems running in a continuous and non-equilibrium\n",
    "fashion is not straightforward. In this educational article, we present a few examples to illustrate\n",
    "how the concept of photon entropy, combined with the radiative transfer equation, can be used to\n",
    "analyze the local entropy generation processes and the efficiency limits of different solar energy\n",
    "conversion systems. We provide explicit calculations for the local and total entropy generation\n",
    "rates for simple emitters and absorbers, as well as photovoltaic cells, which can be readily\n",
    "reproduced by students. We further discuss the connection between the entropy generation and the\n",
    "device efficiency, particularly the exact spectral matching condition that is shared by infinitejunction photovoltaic cells and reversible thermoelectric materials to approach their theoretical\n",
    "efficiency limit.\"\"\"\n",
    "\n",
    "encoded_input = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "\n",
    "print(len(text.split()))\n",
    "print(encoded_input['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDCvvBEW8t7w"
   },
   "source": [
    "Why does the `encoded_input` have more elements than the actual number of words in `text`?\n",
    "\n",
    "1. Subword Tokenization: BERT uses a subword tokenization algorithm where words are often broken down into smaller subwords. This helps the model deal with unfamiliar words while gaining a better understanding of linguistics.\n",
    "2. Special Tokens: BERT requires certain special tokens, including:\n",
    "    a. [CLS]: Special token added at the beginning of input sequences. Output representation is summary of entire text sequence.\n",
    "    b. [SEP]: Special token indicating the end of a sentence or the separation between two sentences.\n",
    "    c. [PAD]: Tokens used for padding sequences to a uniform length.\n",
    "3. Word vs. Token Count: The method len(text.split()) counts the number of words separated by whitespace in the input text. However, the BERT tokenizer counts tokens which include subwords and special characters. Therefore, the number of tokens is typically higher than the number of words separated by whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGFYF1Mg8t7y"
   },
   "source": [
    "### 2.3 ~ Output Word Vectors from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Erf0c1KW8t7y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 275, 768])\n"
     ]
    }
   ],
   "source": [
    "output = model(**encoded_input)\n",
    "\n",
    "last_hidden_state = output[0]\n",
    "\n",
    "print(last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3L9vmOBu8t7z"
   },
   "source": [
    "With the following code, we can find the corresponding token of each integer id in `input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "59jfY6DG8t7z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1996, 2980, 2791, 1997, 1996, 3103, 1998, 1996, 3147]\n",
      "['[CLS]', 'the', 'hot', '##ness', 'of', 'the', 'sun', 'and', 'the', 'cold']\n"
     ]
    }
   ],
   "source": [
    "input_ids_pt = encoded_input['input_ids']\n",
    "input_ids_list = input_ids_pt.tolist()[0]\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids_list)\n",
    "\n",
    "print(input_ids_list[:10])\n",
    "print(input_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1HiiCNT8t70"
   },
   "source": [
    "Can we find the output vector**s** among `last_hidden_state` that correpond to the input word \"entropy\"?\n",
    "Do they have the same values?\n",
    "\n",
    "We can use a `if` statement to check if the current token is the word \"entropy\", and if so,  can append it to `vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "-AMwmmSr8t70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"entropy\": 6\n",
      "Do they have the same value? [False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "for i, token in enumerate(input_tokens):\n",
    "    if token == \"entropy\":\n",
    "        vector = last_hidden_state[0, i]\n",
    "        vectors.append(vector)\n",
    "print('Number of \"entropy\":', len(vectors))\n",
    "\n",
    "matches = [torch.allclose(vectors[i], vectors[i+1]) for i in range(len(vectors)-1)]\n",
    "print(f'Do they have the same value? {matches}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySpNfE9I8t71"
   },
   "source": [
    "### 2.4 ~ Sentence vectors from BERT\n",
    "\n",
    "We can obtain the output vectors for a batch of sentences.\n",
    "\n",
    "First, we need to break the text into a list of sentences, using a simple end-of-sentence str '.' as a separater. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "G9SbxEgO8t71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting in 6 sentences:\n",
      "['The hotness of the sun and the coldness of the outer space are inexhaustible thermodynamic resources for human beings.', 'From a thermodynamic point of view, any energy conversion systems that receive energy from the sun and/or dissipate energy to the universe are heat engines with photons as the \"working fluid\" and can be analyzed using the concept of entropy.', 'While entropy analysis provides a particularly convenient way to understand the efficiency limits, it is typically taught in the context of thermodynamic cycles among quasi-equilibrium states and its generalization to solar energy conversion systems running in a continuous and non-equilibrium fashion is not straightforward.', 'In this educational article, we present a few examples to illustrate how the concept of photon entropy, combined with the radiative transfer equation, can be used to analyze the local entropy generation processes and the efficiency limits of different solar energy conversion systems.', 'We provide explicit calculations for the local and total entropy generation rates for simple emitters and absorbers, as well as photovoltaic cells, which can be readily reproduced by students.', 'We further discuss the connection between the entropy generation and the device efficiency, particularly the exact spectral matching condition that is shared by infinitejunction photovoltaic cells and reversible thermoelectric materials to approach their theoretical efficiency limit.']\n"
     ]
    }
   ],
   "source": [
    "sentences = text.replace('\\n', ' ').split('.')\n",
    "sentences = [s.strip() + '.' for s in sentences if len(s.strip())>0]\n",
    "\n",
    "print(f'Resulting in {len(sentences)} sentences:')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBT2qJOw8t71"
   },
   "source": [
    "Now, let's use tokenizer on this batch of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "O3ZqqmVg8t72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 57])\n",
      "tensor([  101,  1996,  2980,  2791,  1997,  1996,  3103,  1998,  1996,  3147,\n",
      "         2791,  1997,  1996,  6058,  2686,  2024,  1999, 10288, 13821,  3775,\n",
      "         3468,  1996, 10867,  7716, 18279,  7712,  4219,  2005,  2529,  9552,\n",
      "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = tokenizer.batch_encode_plus(sentences, padding=True, return_tensors='pt')\n",
    "\n",
    "print(encoded_sentences['input_ids'].shape)\n",
    "print(encoded_sentences['input_ids'][0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYA7h9OO8t72"
   },
   "source": [
    "We can find that shorter sentences are padded with a special id `0`.\n",
    "\n",
    "Next, we can obtain the output tensors for all input sentences, also in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Ti6zE5Ng8t72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 57, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**encoded_sentences)\n",
    "\n",
    "print(outputs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcLgx9xI8t72"
   },
   "source": [
    "Note that the first dimension of `outputs['last_hidden_state']` is batch size.\n",
    "\n",
    "So the output tensor for the 1st sentence is `outputs['last_hidden_state'][0]`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "kV2WoMCU8t73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3e9KZx28t73"
   },
   "source": [
    "For each output tensor, the first 768-dim vector (at position 0) always corresponds to the special input token `[CLS]`. \n",
    "\n",
    "We can use this vector to represent the meaning of the whole sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "cim3ChPi8t73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "CLS_vec = outputs[0][0][0]\n",
    "print(CLS_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWopMVKR8t74"
   },
   "source": [
    "Now, we have to compute the cosine similarities between each pair of the 6 sentences, and find the pair that has the closest meanings.\n",
    "\n",
    "We can use the `cosine_similarity()` function imported at the beginning, which takes input two tensors and returns the similarity score in a tensor. So we will need to append a `.item()` to retrieve the numeric value from the returned tensor. We also need to specify the argument `dim=0`.\n",
    "\n",
    "***Note***: when calling cosine_similarity() function, remember to specify dim=0; also need to append .item() at the end to obtain a number instead of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "v5OQcKmD8t74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <-> 1: 0.8591641187667847\n",
      "0 <-> 2: 0.777198314666748\n",
      "0 <-> 3: 0.7985227108001709\n",
      "0 <-> 4: 0.7754687666893005\n",
      "0 <-> 5: 0.8052165508270264\n",
      "1 <-> 2: 0.876341700553894\n",
      "1 <-> 3: 0.832162082195282\n",
      "1 <-> 4: 0.8238449096679688\n",
      "1 <-> 5: 0.8492753505706787\n",
      "2 <-> 3: 0.8241375088691711\n",
      "2 <-> 4: 0.8598625659942627\n",
      "2 <-> 5: 0.8579832911491394\n",
      "3 <-> 4: 0.9018083214759827\n",
      "3 <-> 5: 0.9291440844535828\n",
      "4 <-> 5: 0.9185266494750977\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for j in range(i+1, 6):\n",
    "        vec_i = outputs[\"last_hidden_state\"][i][0]\n",
    "        vec_j = outputs[\"last_hidden_state\"][j][0]\n",
    "        sim = cosine_similarity(vec_i, vec_j, dim=0).item()\n",
    "        print(f'{i} <-> {j}: {sim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf5xshFV8t74"
   },
   "source": [
    "We can print out the two sentences to see if the similarity score makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "1BhZGMKc8t74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this educational article, we present a few examples to illustrate how the concept of photon entropy, combined with the radiative transfer equation, can be used to analyze the local entropy generation processes and the efficiency limits of different solar energy conversion systems.\n",
      "We further discuss the connection between the entropy generation and the device efficiency, particularly the exact spectral matching condition that is shared by infinitejunction photovoltaic cells and reversible thermoelectric materials to approach their theoretical efficiency limit.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[3])\n",
    "print(sentences[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngfL2vK38t75"
   },
   "source": [
    "### 2.5 ~ Play with summarization\n",
    "\n",
    "Lastly, let's play with the summarization pipelien provided by transformers. Be patient when the model is downloading.\n",
    "\n",
    "We can try the following code with different input text or arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ro8Ay1688t75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' The hotness of the sun and the coldness of outer space are inexhaustible thermodynamic resources for human beings . From a thermodynamic point of view, any energy conversion systems that receive energy from the sun or dissipate energy to the universe are heat engines with photons as the \"working fluid\"'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "print(summarizer(text, max_length=150, min_length=30))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
