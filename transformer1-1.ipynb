{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q375Ssbh8ap0"
   },
   "source": [
    "# **Transformer and Transformer-Based Models (Part 1)**\n",
    "\n",
    "In this python notebook, we will implement the **multiple head attention** sub layer in a transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GPDPovUW8ap4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.special import softmax\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwYXNhJe8ap6"
   },
   "source": [
    "***\n",
    "\n",
    "## **1. Implement the Multiple Head Attention Sub-Layer**\n",
    "\n",
    "### 1.1 ~ Initialize Input Data\n",
    "\n",
    "Step 1, we generate some random input data in the shape of $\\text{n\\_inputs}\\times \\text{d\\_model}$. \n",
    "\n",
    "We use `np.random.rand()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "2eobFj7i8ap6"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "d_model = 512\n",
    "n_inputs = 3\n",
    "\n",
    "x = np.random.rand(n_inputs, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Cnu5-K1L8ap7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[0.5488135  0.71518937 0.60276338 ... 0.44613551 0.10462789 0.34847599]\n",
      " [0.74009753 0.68051448 0.62238443 ... 0.6204999  0.63962224 0.9485403 ]\n",
      " [0.77827617 0.84834527 0.49041991 ... 0.07382628 0.49096639 0.7175595 ]]\n",
      "x.shape: (3, 512)\n"
     ]
    }
   ],
   "source": [
    "print('x:', x)\n",
    "print('x.shape:', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIXWoa1X8ap8"
   },
   "source": [
    "### 1.2 ~ Create Weight Matrices for *query*, *key*, and *value*\n",
    "\n",
    "Step 2, we create the weight matrices into the correct dimensions. \n",
    "\n",
    "Let's start with `W_query` and `Q`. \n",
    "\n",
    "We first initialize an empty tensor `W` in the dimension of `(d_model, d_k)`, using the `torch.empty()` function.\n",
    "\n",
    "Then we initialize it with `nn.init.xavier_uniform_()`.\n",
    "\n",
    "After `W_query` is initialized, we can get the query matrix `Q` with a multiplication between `x` and `W_query`. \n",
    "\n",
    "We use `np.matmul()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "NYS70w2E8ap9"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "n_heads = 8\n",
    "d_k = d_model // n_heads\n",
    "\n",
    "# Create an empty tensor W with the correct dimension.\n",
    "W = torch.empty(d_model, d_k)\n",
    "\n",
    "# Randomly initialize the values in the tensor.\n",
    "nn.init.xavier_uniform_(W)\n",
    "# Copy out the numpy array\n",
    "W_query = W.data.numpy()\n",
    "\n",
    "Q = np.matmul(x, W_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "nR6gl2Wr8ap-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query[0,:5]: [-0.00076412  0.05475055 -0.0840017  -0.07511146 -0.03930965]\n",
      "W_query.shape: (512, 64)\n",
      "Q[0, :5]: [-0.22772416  0.48167867  1.48693414 -1.00410582  0.19323682]\n",
      "Q.shape: (3, 64)\n"
     ]
    }
   ],
   "source": [
    "print('W_query[0,:5]:', W_query[0,:5])\n",
    "print('W_query.shape:', W_query.shape)\n",
    "print('Q[0, :5]:', Q[0,:5])\n",
    "print('Q.shape:', Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4M_xE7qW8ap_"
   },
   "source": [
    "Next, repeat for `W_key` & `K`, and `W_value` & `V`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "3gxV7dCf8aqA"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "W = torch.empty(d_model, d_k)\n",
    "\n",
    "nn.init.xavier_uniform_(W)\n",
    "W_key = W.data.numpy()\n",
    "\n",
    "K = np.matmul(x, W_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "BFjO91aB8aqA"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "\n",
    "W = torch.empty(d_model, d_k)\n",
    "\n",
    "nn.init.xavier_uniform_(W)\n",
    "W_value = W.data.numpy()\n",
    "\n",
    "V = np.matmul(x, W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "6tF9bStn8aqB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K[0,:5] [ 0.22836541 -0.65482718 -0.07202062  0.49886369  0.5704503 ]\n",
      "K.shape (3, 64)\n",
      "V[0,:5] [-0.44997758  0.92097353 -0.76932045  0.03289758 -0.49462581]\n",
      "V.shape (3, 64)\n"
     ]
    }
   ],
   "source": [
    "print('K[0,:5]', K[0,:5])\n",
    "print('K.shape', K.shape)\n",
    "print('V[0,:5]', V[0,:5])\n",
    "print('V.shape', V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5_Rm_cD8aqC"
   },
   "source": [
    "### 1.3 ~ Compute Attention Scores and Weighted Output\n",
    "\n",
    "Step 3, we compute the attension scores using the matrices `Q` and `K`, following the equation:\n",
    "\n",
    "\\begin{equation}\n",
    "Attention(Q, K, V) = softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_k}})V\n",
    "\\end{equation}\n",
    "\n",
    "in which $\\sqrt{d_k}$ is for normalization purpose.\n",
    "\n",
    "We should first compute `attn_scores`, which is the unnormalized score. Then we can apply the `softmax()` function imported from `scipy` to calculate the normalized scores. Note that we need to specify the `axis` argument correctly when we call `softmax()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "2yBs2CjJ8aqC"
   },
   "outputs": [],
   "source": [
    "attn_scores = np.dot(Q, K.T) / math.sqrt(d_k)\n",
    "\n",
    "attn_scores_norm = softmax(attn_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "PDbykYOp8aqC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores.shape: (3, 3)\n",
      "Unnormalized attn_scores: [[-0.75497316 -0.97036241 -0.85112739]\n",
      " [ 0.2377701  -0.70730389 -0.37639248]\n",
      " [ 0.21608568 -0.73905382 -0.89881122]]\n",
      "Normalized atten_scores: [[0.36838498 0.29700213 0.33461289]\n",
      " [0.51820328 0.20140013 0.2803966 ]\n",
      " [0.58387084 0.22464925 0.19147991]]\n"
     ]
    }
   ],
   "source": [
    "print('attn_scores.shape:', attn_scores.shape)\n",
    "print('Unnormalized attn_scores:', attn_scores)\n",
    "print('Normalized atten_scores:', attn_scores_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-NXVAX8aqD"
   },
   "source": [
    "Step 4, finally, we compute the output as the weighted sum of value (`V`), using the above computed `attn_scores_norm` as the weight.\n",
    "\n",
    "`attn_scores_norm[0,:]` is the weight for the first output `weighted_output[0,:]`, \\\n",
    "so the computation is:\\\n",
    "`weighted_output[0,:] = attn_scores_norm[0,0] * V[0,:] + attn_scores_norm[0,1] * V[1,:] + attn_scores_norm[0,2] * V[2,:]`\n",
    "\n",
    "But we can achieve this with one line code using `np.matmul()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "3qajj14q8aqD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted_output[0,:5]: [-0.37040035  0.49331394 -0.78595571  0.09711597 -0.33551546]\n",
      "weighted_output.shape: (3, 64)\n"
     ]
    }
   ],
   "source": [
    "weighted_output = np.matmul(attn_scores_norm, V)\n",
    "\n",
    "print('weighted_output[0,:5]:', weighted_output[0,:5])\n",
    "print('weighted_output.shape:', weighted_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMcnfq6j8aqE"
   },
   "source": [
    "***\n",
    "\n",
    "**We have finished Task 1, and now we know how to implement the self-attention module, which is the core technique of Transformer.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
